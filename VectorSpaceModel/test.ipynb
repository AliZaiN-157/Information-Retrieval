{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stopwords = []\n",
    "N = 20\n",
    "\n",
    "\n",
    "def load_stopwords():\n",
    "    with open('Stopword-List.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            # if line is space, skip\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            stopwords.append(line.strip())\n",
    "\n",
    "\n",
    "load_stopwords()\n",
    "\n",
    "\n",
    "def preprocessing(corpus):\n",
    "    # lowercase\n",
    "    corpus = corpus.lower()\n",
    "    # remove punctuation\n",
    "    corpus = re.sub(r'[^\\w\\s]', '', corpus)\n",
    "    # remove numbers\n",
    "    corpus = re.sub(r'\\d+', '', corpus)\n",
    "    # replace multiple spaces with single space\n",
    "    corpus = re.sub(r'\\s+', ' ', corpus)\n",
    "    # remove leading and trailing spaces\n",
    "    corpus = corpus.strip()\n",
    "    # remove irrelevant characters\n",
    "    corpus = re.sub(r'[^\\x00-\\x7F]+', '', corpus)\n",
    "    tokens = word_tokenize(corpus)\n",
    "    # remove stopwords and stem\n",
    "    tokens = [ps.stem(token) for token in tokens if token not in stopwords]\n",
    "    # remove single character tokens\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    # remove large tokens\n",
    "    tokens = [token for token in tokens if len(token) < 20]\n",
    "    # remove tokens with consecutive characters\n",
    "    tokens = [token for token in tokens if not re.match(\n",
    "        r\".*(.)\\1{2,}.*\", token)]\n",
    "    # remove urls with http or https using startswith\n",
    "    tokens = [token for token in tokens if not token.startswith(\n",
    "        'http') and not token.startswith('https')]\n",
    "    # http or https in the middle of the url\n",
    "    tokens = [token for token in tokens if not re.match(\n",
    "        r\"[a-zA-Z0-9\\./]+http[a-zA-Z0-9\\./]+\", token)]\n",
    "    # remove url with github\n",
    "    tokens = [token for token in tokens if not re.match(\n",
    "        r\"github/[a-zA-Z0-9\\./]+\", token)]\n",
    "    # remove email addresses using regex\n",
    "    tokens = [token for token in tokens if not re.match(\n",
    "        r\"[^@]+@[^@]+\\.[^@]+\", token)]\n",
    "    return tokens\n",
    "\n",
    "doc_ids = []\n",
    "\n",
    "def load_data():\n",
    "    data = []\n",
    "    for filename in sorted(os.listdir(r'../ResearchPapers'), key=lambda x: int(x[:-4])):\n",
    "        with open(r'../ResearchPapers/' + filename, 'r') as f:\n",
    "            doc_ids.append(int(filename[:-4]))\n",
    "            data.append(f.read())\n",
    "    return data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [terms, 1, 2, 3, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, df, idf]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 23 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = [\"terms\",*doc_ids, 'df', 'idf']\n",
    "df = pd.DataFrame(columns=index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_tf_idf(df,data):\n",
    "    for i, doc in enumerate(data):\n",
    "        tokens = preprocessing(doc)\n",
    "        token_count = Counter(tokens)\n",
    "        for token, count in token_count.items():\n",
    "            if token not in df['terms'].values:\n",
    "                df = df._append({'terms': token}, ignore_index=True)\n",
    "            df.loc[df['terms'] == token, doc_ids[i]] = count\n",
    "    df = df.fillna(0)\n",
    "    df['df'] = df[doc_ids].apply(lambda x: sum(x > 0), axis=1)\n",
    "    df['idf'] = df['df'].apply(lambda x: math.log10(N/x))\n",
    "    for doc_id in doc_ids:\n",
    "        # df[doc_id] = df[doc_id].apply(lambda x: 1 + math.log10(x) if x > 0 else 0)\n",
    "        df[doc_id] = df[doc_id] * df['idf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Zain\\AppData\\Local\\Temp\\ipykernel_3056\\2945057573.py:15: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved to file\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('tf_idf_new.csv'):\n",
    "    print('Loading index from file')\n",
    "    new_df = pd.read_csv('tf_idf_final.csv')\n",
    "else:\n",
    "    print('Computing index')\n",
    "    new_df = compute_tf_idf(df,data)\n",
    "    new_df.to_csv('tf_idf_new.csv',index=False)\n",
    "    print('Index saved to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_query_tf_idf(query):\n",
    "    global new_df\n",
    "    tokens = preprocessing(query)\n",
    "    token_count = Counter(tokens)\n",
    "    for token, count in token_count.items():\n",
    "        if token not in new_df['terms'].values:\n",
    "            new_df = new_df._append({'terms': token}, ignore_index=True)\n",
    "        new_df.loc[new_df['terms'] == token, 'query'] = count\n",
    "    new_df = new_df.fillna(0)\n",
    "    # new_df['query'] = new_df['query'].apply(\n",
    "    #     lambda x: 1 + math.log(x, 10) if x > 0 else 0)\n",
    "    new_df['query'] = new_df['query'] * new_df['idf']\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# def create_vector():\n",
    "#     global new_df\n",
    "#     float_cols = new_df.select_dtypes('float64').columns\n",
    "#     vector = {}\n",
    "#     for col in float_cols:\n",
    "#         vector[col] = new_df[col].values\n",
    "#     vector.pop('idf')\n",
    "#     query_vector = vector.pop('query')\n",
    "#     new_df = new_df.drop('query', axis=1)\n",
    "#     # print(new_df.head(10))\n",
    "#     return vector, query_vector\n",
    "\n",
    "def create_vector():\n",
    "    global new_df\n",
    "    vec = {}\n",
    "    for id in doc_ids:\n",
    "        vec[id] = new_df[id].values\n",
    "    query_vector = new_df.pop(\"query\").values\n",
    "    return vec, query_vector\n",
    "\n",
    "\n",
    "def cosine_similarity(query_vector, doc_vector):\n",
    "    dot_product = sum(query_vector * doc_vector)\n",
    "    query_norm = math.sqrt(sum(query_vector ** 2))\n",
    "    doc_norm = math.sqrt(sum(doc_vector ** 2))\n",
    "    return dot_product / (query_norm * doc_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 7, 16, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "def queryFetcher(query):\n",
    "    # query = \"information retrieval\"\n",
    "    new_df = add_query_tf_idf(query)\n",
    "    vector, query_vector = create_vector()\n",
    "    # calculate cosine similarity\n",
    "    cosine_sim = {}\n",
    "    for doc_id, doc_vector in vector.items():\n",
    "        cosine_sim[doc_id] = cosine_similarity(query_vector, doc_vector)\n",
    "    # sort the dictionary by values by a threshold of 0.03\n",
    "    cosine_sim = {k: v for k, v in sorted(\n",
    "        cosine_sim.items(), key=lambda item: item[1], reverse=True) if v > 0.03}\n",
    "    return list(cosine_sim.keys())\n",
    "\n",
    "print(queryFetcher(\"machine learning\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ouput on Alpha value = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning ['24', '7', '16', '2', '1']\n",
    "# intelligent search ['7', '3', '1', '2']\n",
    "# cancer NIL\n",
    "# deep convolutional network ['16', '3', '2', '7']\n",
    "# artificial intelligence ['1', '8']\n",
    "# transformer ['21', '18']\n",
    "# local global feature ['22', '23', '24', '25', '26', '7']\n",
    "# feature selection machine learning ['22', '24', '23', '25', '26', '7', '1']\n",
    "# information retrieval ['1']\n",
    "# natural intelligence ['7', '2', '3', '1']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
